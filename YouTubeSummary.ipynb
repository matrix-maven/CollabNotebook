{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOp/GvkOkGhXw5D8MCZcIOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matrix-maven/CollabNotebook/blob/main/YouTubeSummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zux8oHhvE9U5",
        "outputId": "12421ea3-9fd5-4a04-adaf-425bfe5f95b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.10.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2024.2.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.6.0)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SET ENVIRONMENT"
      ],
      "metadata": {
        "id": "7fUKa1qvQlnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n"
      ],
      "metadata": {
        "id": "LIT7Ps1eO1ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Transcript Function"
      ],
      "metadata": {
        "id": "m_UNctOfIkID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_english_transcript(transcript_list):\n",
        "  \"\"\"\n",
        "  Returns the English transcript if available, otherwise None.\n",
        "\n",
        "  Args:\n",
        "    transcript_list: A list of Transcript objects.\n",
        "\n",
        "  Returns:\n",
        "    The English transcript if available, otherwise None.\n",
        "  \"\"\"\n",
        "  for transcript in transcript_list:\n",
        "    if transcript.is_generated == False:\n",
        "      print(\"Manual Transcript Language: \" +transcript.language)\n",
        "      return transcript.fetch()\n",
        "  print('Automated Transcript Language: '+transcript.language)\n",
        "  return transcript.fetch()\n"
      ],
      "metadata": {
        "id": "qMbSwprKIfMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pass Yourtube URL"
      ],
      "metadata": {
        "id": "2scziujjVLMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_video_url = 'https://www.youtube.com/watch?v=kAgSPzlgnDM&t=204s'  # @param {type: \"string\"}\n",
        "try:\n",
        "        video_id=youtube_video_url.split(\"=\")[1]\n",
        "        print(video_id)\n",
        "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "        #print(transcript_list)\n",
        "        # iterate over all available transcripts\n",
        "        transcript_text = get_english_transcript(transcript_list)\n",
        "        transcript = \"\"\n",
        "        #print(transcript_text)\n",
        "        for i in transcript_text:\n",
        "            transcript += \" \" + i[\"text\"]\n",
        "        print(transcript)\n",
        "\n",
        "except Exception as e:\n",
        "        raise e\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZuUrWNs2ghr",
        "outputId": "49d0107e-d945-41e3-8c52-b48d5cba3e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kAgSPzlgnDM&t\n",
            "Automated Transcript Language: English (auto-generated)\n",
            " hey folks I'm Eric at Lang chain and today we're going to be talking about using mongodb semantic caching in order to cut our llm costs today mongodb launched their integration package with Lang chain so you can use mongodb as a vector store or as a cache or even a semantic cache which we're going to go through today uh we're going to be using a few different providers in our solution um so the first thing that we're going to go through is kind of who who each of them are uh then we're going to go through two examples from our cookbook one just being a basic prompt in llm chain with semantic caching and one with rag where we'll actually discuss some of the challenges of using semantic caching with a retrieval based system so without further Ado let's dive in the first orchestration framework that we're going to be using is Lang chain the company that I work at UH these are the Lang chain docs we allow you to connect to any llm any Vector store uh different kinds of tools and we're kind of a One-Stop shop for all those different Integrations and then we provide some standard abstractions uh for those Integrations as well as some other approaches uh we have lots of cookbooks that allow you to implement academic papers or just kind of basic Solutions which we're going to be going through today um the next up is mongodb specifically mongodb is a nosql database provider that also provides Atlas which is a cloud database available in a few different uh environments it has native Vector search uh which is the reason that we're going to be using it today cement de caching involves uh kind of an embedding lookup very akin to a vector store retrieval step so we're going to be leveraging the atlas Vector search instance there uh and we're also going to be using uh some other providers as well so we're going to be using anthropics uh Cloud 3 llm in order to uh actually generate some responses from our application we're going to be using open AIS embeddings uh for this particular application and when we get to the rag step we're going to be using X's search retriever in order to retrieve some documents to actually base our responses on so let's dive in the first chain that we're going to implement is the basic prompt and llm chain from our cookbook it's the first entry and rag conveniently is the second one as well the idea uh behind this is it's it's really the simplest thing that you can do for getting started and so we're going to make something kind of similar to that so let's go into our editor here um so the first thing you're going to want to do is you're going to want to install some packages so today we're going to be using um four main providers mongod DB anthropic open Ai and EXA so we're going to want to install those for partner packages so we can see those up here where you're going to pip install Lang chain mongodb Lang chain anthropic Lang chain open Ai and Lang chain XA uh the next step is we're going to want to set some environment variables in order to get those set up I already have those set up in my environment so we're not going to have to worry about those um we're going to be using open AI for embeddings in this particular example uh and then last but not least uh Lang Smith is our kind of first party observability and debugging tool at Lang chain um and if you want to use that in order to debug traces um we can actually just quickly open that to see what that looks like um and just briefly showing what lsmith looks like we'll look at this a little bit more later with some of the further examples it allows us to trace all of our different um applications in order to see kind of how the prompt is constructed and which documents are actually retrieved as part of different retrieval steps uh as well as uh other things such as kind of llm pricing uh token counting those kinds of things so we'll be using that um down here and you just need to set these two environment variables to activate that throughout kind of your entire Lane chain application which is quite nice so once we have all of the that set up we can dive in so first thing we're going to do is we're going to want to set up some sort of prompt into model chain um so the kind of way that we compose prompts and models as well as everything in L chain is kind of through this runnable interface where we can pipe the output of a prompt into a model now let's configure what that prompt and model actually are so first we're going to want to set up our model uh and we're going to use a chat anthropic model using their new Cloud 3 uh Sonet model so we'll say from blank chain anthropic import chat anthropic um and actually let's use Opus instead just so we can show off some of the speed gains as well from caching so uh Opus is anthropics largest and slowest Cloud 3 model um if you want faster ones you can obviously use Sonet and Haiku as well so we'll use cloud 3 uh Opus 24229 they have their nice leape date in their model uh and then we'll also want to set up our prompt so in order to make a prompt we'll do from Lang chain core. prompt import prompt template and we'll set our prompt to be uh promp template. from template and we'll give it a nice little string here and we'll do actually we'll start with just a classic rag prompt where we'll say answer the following question about the given context and we'll give some kind of XML surrounded context so the squiggly bracket context field is what we're actually going to substitute in as part of of uh our prompt template and then these XML tags are actually just there to indicate to the llm that that is what the context is and then we'll give a question below there um so that should form our prompt and our model and we'll see if that runs looks like we successfully did that and then we can now try invoking it um with some sort of context and some sort of question where we are essentially substituting in those two kind of blue variables from up here um from whatever we set it to so we could for example give it something like Eric works at Lang chain with a question of where does Eric work and we can see what we get back from that and in 2.6 seconds again Opus is kind of a larger model we can see that BAS on the given context Eric works at L chain so looks like our our basic chain is working um if I were to run this again it should still take two or seconds uh because it's going to actually send that API call to anthropic every single time that we do this uh as well as rack up token usage as well as our our good anthropic Bill there um so in order to kind of cut some of that let's go into semantic caching um so caching in general um and I'll actually draw a bit of a diagram here um semantic caching in general uh intends to kind of uh prevent us from sending too many API calls to our model provider so in the chain that we just set up where we have a prompt going into a model and giving some kind of output what we're essentially going to do is we're going to create a kind of bypass path around the model step where if we've seen kind of the formatted prompt before we're actually just going to grab that from our cache so in this case uh we're going to be doing that with kind of a  DB based cache um and we're going to just get whatever response we had last time from that um and there's kind of two main ways that we can do that so the the first is with a regular cache where we're going to be looking for an exact match of the formatted prompt and this is good for instances where we know we're going to get very similar prompts uh from whatever system we're using so in our particular example this actually likely won't work particularly well because it's passing in uh a user's question as part of it and then likely that context is coming from some sort of retrieval step which we're going to add in a second um and so if I asked even something just slightly different such as uh where did Eric work uh period instead of a question mark it's actually going to not be able to look that up because it needs that exact match in order to look it up and so the approach that we're going to be using today in order to make that work is called semantic caching where instead of indexing by the kind of exact formatted prompt string we're going to index by an embedding of it so we're going to use embeddings plus a vector store uh so we're going to use Vector search uh to look up matches and we're going to set some sort of threshold because an important part of of these systems is that if there isn't something with sufficient similarity we actually do still want to call the model um and then of course at the end of this we're also going to want to insert our record back into our database uh such that we can actually access that the next time that we call it so today we're going to be focused on the semantic cache implementation of um and we also have documentation on how to use the regular cache if that works better for your use case so let's get started on adding semantic caching to our kind of basic prompt and model chain um so to do that we're going to first create our semantic cache instance so we're going to import that from the Lang chain mongodb package and it's called the mongod DB Atlas semantic cache um is what we're going to import uh this relies on mongod DBS hosted Atlas solution in order to enable Vector search so note that if you're using open source uh you might run into some problems using this um and then we're also going to want to configure which embeddings we want to use so in our case we're going to import that from Lang chain open AI import open AI embeddings um you can use lots of local ones from hugging face nomic has some pretty good embeddings um Voyage AI has a new embedding integration with us that really excited about so you can kind of pick your embedding provider we're just use open AI for convenience today um and then we're going to also import our good OS package so we can access some environment variables um I'm actually going to paste in some of the configuration that we're going to use for our uh Vector search today uh the first is we're going to load in kind of my secret atas URI uh as our connection string this is Mongo's way of configuring kind of with Atlas instance to connect to and it also encodes the password used um to connect to it so I have that set as an environment variable already um and then we're going to configure an index database and uh collection to connect to as well so we can try running that to make sure we can actually load that and then we're going to want to create our semantic cache as our semantic cache instance um we'll create our embedding as opening embeddings and we'll use their uh text embedding three small model in order to do that we don't want to add any more dashes to that and then we'll have our connection string as con string we'll have our collection name as collection our database name and our index name in order to kind of wrap that up and then last but definitely not least we're also going to want to configure a threshold in order to make sure we're not returning things that are potentially the most relevant uh entry even though it's not relevant at all um so that's something that we do in classic retrieval where we actually always want to get kind of the three most relevant documents from our Vector store um but with semantic caching even if there are three entries that are the most relevant to our query um it's possible that those three entries are actually not relevant at all to us so uh this particular index uses a cosine distance um or a cosine relevant score and so we'll set um our threshold to make sure it's greater than 0.99 in order to um get that set up so we can try running this make sure I didn't have any typos um and then we should be good to go so the first thing that we'll want to do is we'll actually just want to clear that cache um so the llm cache abstraction in Lang chain allows you to clear uh kind of all the entries in it and so we'll do that just to make sure there's there's nothing in there to start um and then we'll want to uh set our llm cache so in Lang chain there's a kind of convenient way of setting that globally so you don't actually have to configure this on each and every llm call that you have so we can have from Lang chain core globals import set llm cache and here we'll just set our LM cache to our semantic cache object um and so this will allow us to cach our responses um in that as well as kind of get our responses back out whenever we've seen um ones that are sufficiently similar through the semantic cache um lookup so we can try our nice question about where I work again uh as expected the first time we run it uh this takes some time where um again based on the given context Eric works at L chain and then if we run it again we should already have an entry in uh our mongodb Atlas semantic cache such that it should be pretty quick so let's try running it again and we get our response much faster um so now let's try changing our query slightly um so let's add some comments here to make that clear same query exact query and then almost same query so we can try an example where um instead where does Eric work question mark we'll do where does Eric work period hopefully that will also be able to retrieve it and then now let's try one that is different um so here we're going to instead ask something about um the mayor of San Francisco is London and breed and here we basically want to make sure um that we are not retrieving our Eric works at Lang chain response um because in that case that would that would really represent a failure of our chain so here hopefully it takes some time so far so good um and it looks like it's successfully making that llm call and now if we ask um some along those lines again it should be quick because it's already cached from this prior step so looks like our cement to cach is working on our basic example um we went through kind of what semantic caching is and why it's powerful um and now let's actually augment our or create a new chain that uh implements rag instead of like explicitly plumbing through the exact context that we have up here so let's go through that and we're going to use our uh EXA search retriever from this so we'll import uh from linkchain EXA import EXA search Retriever and uh we'll also import a a configuration option called text contents options um we can actually pull up the ex search retriever documentation and the reason we're importing that is um because we want to configure kind of a max length uh for each of the documents that it returns so EXA is an integration that searches uh the internet much like Google um and returns documents from their index and so some of the documents on the internet are quite long so we're going to want to set kind of a max length for that so let's create our retriever um I have my exit API key already set and we're going to want to set our text content options to text content options of Max characters as uh let's start with 300 for now um depending on the use case and the types of questions we expect uh that might be a little bit short but we can give it a go for now and then we'll kind of create our uh chain from scratch again so we'll actually use the um the prompt we already had because it plums in context and question um um but we will start with a different step where we actually need to populate our question which we can do as a runnable pass through and we'll have to import that from linkchain core runnables um so that's just going to pass through the exact um string that we invoke our chain with through the question and then context we're actually just going to pass that string into our Retriever and then we can pass pass that into a kind of format documents function which we'll Define up here um where that'll take in docks and then we'll return um well first we can get the contents of all those docs so we'll get doc. page content for Doc and docs and then we will join those with the lines so we'll just have a double new line and we'll join all of those kind of content strings uh so this is just kind of explicitly formatting that into a string if you were to just pass in the documents directly you might get some of the metadata that XO returns kind of plumbed into your llm as well um which in this case is probably not as desirable because that could be ordered differently and especially since we're dealing with caching it's better to just have kind of the whole string in there so let's have that as the first part of our chain and then we'll pass that into our prompt that we had above and then we'll actually use the same model as well so as a reminder this uh chain and we can actually draw this that might be a little bit more clear um this chain is basically adding to the beginning of this one where before this we want to populate both a question as well as some context where the input before that is going to be coming in as just the question so the first thing we're going to do is we're just going to Plum our question straight through that's all the runnable pass through does and down here we're going to pass our question to our XA search Retriever and we're going to pass the output of that into format docs in order to get our context so here again we're kind of passing our question in as input to exess search retriever and then in the middle here this is going to be our kind of list of documents is what's going to be kind of passed through in this middle step and then format docs is going to just turn this into a string where it just has all the contents kind of put together uh our prompt is going to turn that into our string of uh answer the following question based on the given context that will become our formatted prompt um and then we're actually just going to use the exact same cache in order to uh get this working so in order to debug this at first a little bit let's um let's actually make the LM cache none um just to kind of make sure our chain is working before we start relying on the cache so down here we will um have this one thing I just realized I forgot to configure is we never configured the number of documents to return so we'll configure that here we'll have it return three documents for each of our queries um and let's see if I defined everything right and now we should be able to invoke it with something like who is the mayor of San Francisco and ideally that will return some sort of document that will be used to answer that question so it looks like we're successfully getting that London breed is the current and 45th mayor of San Francisco um and if we look at our Langs Smith Trace we should be able to see uh some of the documents that actually get returned from that retriever um where hopefully so we can see the first one seems to be getting a little bit of a tabular bio uh this looks like it might be coming from Wikipedia which looks like correct from down here um here's a document that is coming from sf.gov and here's one that's also coming from sf.gov and they kind of unanimously declare that London breed is the air of San Francisco which is good and correct for our cases um so now let's add back our semantic cache um of our semantic cache and let's actually let's to clear it just to be safe uh because we don't want it to Cache some of the prior responses that we had um and now let's try running this again where we can ask who is the mayor of San Francisco um it's going to run the retrieval step which is not cash one of the key um things to remember is that the um the retrieval step is not cached as part of the LM cache it is only the input and output to the model um and let me make sure I'm not going to completely run out of battery um so we get that our mayor is lon breed and then if we run that again uh so this will not experience as much of a speed of a performance speed up as our prior example because we still need to run that retrieval step because that one is not cached so if we run that again um it actually looks like the retrieval step is pretty quick but then we're still spending uh 08 seconds on that retrieval step and then the uh output of anthropic is uh going to be exactly the same just because our retriever is getting kind of those exact three documents again because it's the exact same query um but then our Chad anthropic step is going to take 32 seconds instead of 6 seconds which it did in the prior step so that is kind of llm caching in a nutshell um now let's talk about some of the challenges with a rag B application and using that in um in an application like this so we can actually change our query here so in our prior example we were able to change our query from where does Eric work to kind of where does Eric work with a period um we can try that one again this one might actually work um so let's see if that does anything yeah so that one still is retrieving the same set of documents but you could also imagine us using some sort of query like um who leads well I guess that's a little bit ambiguous but the mayor of San Francisco is whom um which semantically is very similar and so if we were to Just Produce an embedding of this string alone it would likely get a lookup but in this case we're actually going to retrieve a different set of documents so if we look at our Langs Smith Trace we're going to see that instead of the kind of Wikipedia article sfgv sfgv our retriever is getting um a different section of the Wikipedia article so it's kind of leading with from Wikipedia um it's the second entry is actually also from Wikipedia looks like this is actually kind of similar to what the first document was in the prior example and then the third document has actually become the second so even slight different ref es where now when we format our prompt template we're going to get this as the input to our llm versus we can actually open the old one uh which is going to be slightly different where here our prompt is going to get formatted as like these entries in that order and because those two docu those two inputs are sufficiently different the semantic cache is not going to be able to look those up so the really important thing to remember there and I'll kind of emphasize this on my diagram is that the semantic cache is only caching over here it's only caching the model inputs and outputs and even though your question might be similar so you could also Imagine a world in which we had a cache that's actually caching this whole flow where if you had a similar enough question it would kind of look up the entire response of the entire chain this is not what llm caching does um this is not llm caching uh this is something you can Implement yourself as well um but the really important thing to remember is because we're relying on the retrieval step to return the exact same set of documents we actually have kind of another point of uh getting a cach Miss uh than if we were to be indexing the entire flow all over um so that's kind of one of the gotchas of using llm caching in a rag system like this um but it is still kind of a cool approach and it works very well with mongodb so kind of inside uh today we went through kind of using semantic caching uh in uh kind of two simple applications the first just being a prompt in llm kind of mimicking some sort of chat GPT or kind of custom prompt chatbot uh and then we went down into a rag implementation where the retrieval step was able to produce uh some cash hits if we gave the same query again but actually slight differences in the query uh even though they were semantically similar resulted in different documents being retrieved and still produced a cash Miss um as one of the gchas of semantic caching thanks for listening uh and have a great rest of your day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CAPTURE YOUTUBE VIDEO and TRANSCRIBE"
      ],
      "metadata": {
        "id": "2Kj5X1KzUAAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_video_url = 'https://www.youtube.com/watch?v=YS0-Qg_wgJA'  # @param {type: \"string\"}\n",
        "try:\n",
        "        video_id=youtube_video_url.split(\"=\")[1]\n",
        "        transcript_text=YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript = \"\"\n",
        "        for i in transcript_text:\n",
        "            transcript += \" \" + i[\"text\"]\n",
        "\n",
        "except Exception as e:\n",
        "        raise e\n",
        "\n"
      ],
      "metadata": {
        "id": "8zah2jW_VRHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "f6f8e7c6-5406-48ad-b51d-5d5e92ffd10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'YouTubeTranscriptApi' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c5f96af582c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-0c5f96af582c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mvideo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myoutube_video_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtranscript_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mYouTubeTranscriptApi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranscript_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'YouTubeTranscriptApi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Summary in the format provided as input"
      ],
      "metadata": {
        "id": "GEQjF1nQYHF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Actor = 'Software Engineer' # @param {type: \"string\"}\n",
        "article_format = 'summary' # @param {type: \"string\"}\n",
        "\n",
        "prompt=f\"\"\"You act like {Actor}.\n",
        "You will be taking the transcript text\n",
        "and providing a detailed article on the entire video.\n",
        "Whereas possible please illustrate with example and code snippet\n",
        "I want the format of the article as {article_format}\"\"\"\n",
        "\n",
        "\n",
        "model=genai.GenerativeModel(\"gemini-pro\")\n",
        "response=model.generate_content(prompt+transcript)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "8du_QGK7YGxH",
        "outputId": "713792c1-1df4-4f32-ae34-c38a2d145e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary**\n",
            "\n",
            "MongoDB has released an integration package with LangChain, allowing users to leverage MongoDB as a vector store, cache, or semantic cache. Semantic caching is a technique that embeds and caches model inputs and outputs to reduce API calls to LLMs (Large Language Models).\n",
            "\n",
            "**Key Points**\n",
            "\n",
            "- **Semantic Caching:** Indexes model inputs by embeddings to handle variations in prompts, ensuring cache utilization even with slightly different queries.\n",
            "- **Challenges with RAG:** Retrieving documents from a retrieval step cannot be cached in the LM (Language Model) cache, potentially leading to cache misses despite semantically similar queries.\n",
            "- **Implementation:** Integrate MongoDB Atlas Semantic Cache with LangChain, set a relevance threshold for cache retrieval, and configure caching behavior.\n",
            "\n",
            "**Example 1: Basic Prompt and LLm Chain with Semantic Caching**\n",
            "\n",
            "- **Setup:** Import necessary packages, configure MongoDB Atlas, and set up the prompt and model (Anthropic Opus).\n",
            "- **Results:** Successful caching of exact match prompts, accelerated response time for subsequent calls.\n",
            "- **Limitations:** Cache misses for slightly different prompts due to reliance on exact match.\n",
            "\n",
            "**Example 2: RAG Chain with Semantic Caching**\n",
            "\n",
            "- **Setup:** Integrate EXA Search Retriever, configure query parameters, and add a runnable pass-through step for question input.\n",
            "- **Results:** Successful caching for identical queries, but cache misses for semantically similar queries due to different document retrieval.\n",
            "- **Challenges:** Semantic caching does not cover the retrieval step, limiting its effectiveness in RAG systems with varying query inputs.\n",
            "\n",
            "**Additional Considerations:**\n",
            "\n",
            "- **Alternative Caching Approach:** Implement a cache that covers the entire chain flow, including the retrieval step, to handle semantically similar queries.\n",
            "- **Evaluation:** Monitor LangSmith traces to analyze caching performance, such as hit rates and token usage.\n",
            "- **Speed Gains:** Opus (Anthropic's largest Cloud 3 model) showcases significant speed improvements when leverage semantic caching.\n"
          ]
        }
      ]
    }
  ]
}